{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Ensure that the French stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('express_multiple.csv', encoding='utf-8')\n",
    "\n",
    "# Select the target documents from the second column\n",
    "texts = df.iloc[:, 1].astype(str)\n",
    "\n",
    "# Preprocess the text data\n",
    "# Load French stopwords\n",
    "stop_words = stopwords.words('french')\n",
    "stop_words.extend(['»', '«', '’', 'L', 'a'])\n",
    "\n",
    "# Tokenize and remove stopwords\n",
    "def preprocess(text):\n",
    "    return [word for word in word_tokenize(text.lower()) if word.isalpha() and word not in stop_words]\n",
    "\n",
    "# Apply preprocessing to each document\n",
    "tokenized_texts = texts.apply(preprocess)\n",
    "\n",
    "# Build the bigram model\n",
    "bigram = Phrases(tokenized_texts, min_count=5, threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "# Apply the bigram model to each document\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "# Apply the bigram model to the tokenized texts\n",
    "bigram_texts = make_bigrams(tokenized_texts)\n",
    "\n",
    "# Create Dictionary and Corpus needed for Topic Modeling\n",
    "id2word = corpora.Dictionary(bigram_texts)\n",
    "corpus = [id2word.doc2bow(text) for text in bigram_texts]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = LdaModel(corpus=corpus,\n",
    "                     id2word=id2word,\n",
    "                     num_topics=20,\n",
    "                     random_state=100,\n",
    "                     update_every=1,\n",
    "                     chunksize=100,\n",
    "                     passes=10,\n",
    "                     alpha='auto',\n",
    "                     per_word_topics=True)\n",
    "\n",
    "# Print the topics\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print the number of documents in the corpus\n",
    "def print_corpus_documents_count(corpus):\n",
    "    print(f\"The number of documents in the corpus is: {len(corpus)}\")\n",
    "# Call the function to print the number of documents\n",
    "print_corpus_documents_count(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print the topic distribution for each document\n",
    "def print_topic_distribution_for_each_document(lda_model, corpus):\n",
    "    for i, row in enumerate(corpus):\n",
    "        doc_topics = lda_model.get_document_topics(row)\n",
    "        print(f\"Document {i} topic distribution: {doc_topics}\")\n",
    "\n",
    "# Call the function to print the topic distribution for each document\n",
    "print_topic_distribution_for_each_document(lda_model, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a new CSV document with topic distributions\n",
    "def append_topic_distribution_to_csv(lda_model, corpus, df, output_filename):\n",
    "    # Get the topic distribution for each document\n",
    "    topic_dist_list = [lda_model.get_document_topics(bow, minimum_probability=0) for bow in corpus]\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    topic_dist_df = pd.DataFrame([{topic: prob for topic, prob in doc} for doc in topic_dist_list])\n",
    "    \n",
    "    # Rename columns to reflect topic names\n",
    "    topic_dist_df.columns = [f'Topic_{col}' for col in topic_dist_df.columns]\n",
    "    \n",
    "    # Merge with the original DataFrame\n",
    "    merged_df = pd.concat([df, topic_dist_df], axis=1)\n",
    "    \n",
    "    # Fill NaN values with 0 (documents may not have all topics)\n",
    "    merged_df = merged_df.fillna(0)\n",
    "    \n",
    "    # Save to a new CSV file\n",
    "    merged_df.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "    print(f\"New CSV with topic distributions saved as '{output_filename}'\")\n",
    "\n",
    "# Call the function to create a new CSV with topic distributions\n",
    "append_topic_distribution_to_csv(lda_model, corpus, df, 'express_multiple_with_topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
